<pre>
import numpy as np
</pre>

- Numpy 라이브러리를 `np`라는 별칭으로 불러옵니다. 

<pre>
class MLP:
    def __init__(self, input_size, hidden_size, output_size):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size

        # 가중치 초기화
        # self.w1_2_3_4= np.random.random((self.input_size, self.hidden_size))
        self.w1_2_3_4 = [[1, 10],[1, 10]]
        # self.w5_6 = np.random.random((self.hidden_size, self.output_size))
        self.w5_6 = [[-40], [40]]

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    def forward(self, X):
        # Propagate inputs through the network
        self.z1_2 = np.dot(X, self.w1_2_3_4)
        self.h = self.sigmoid(self.z1_2)
        self.z3 = np.dot(self.h, self.w5_6)
        self.o = self.sigmoid(self.z3)
        return self.o

    def mse_loss(self, y_true, y_pred):
        # MSE 손실계산
        return np.mean((y_true - y_pred) ** 2)

    def backward(self, x, y, y_pred, learning_rate):
        # 체인룰 계산
        dc_do1 = -2 * (y - y_pred)
        do1_dz3 = y_pred * (1 - y_pred)
        dz3_dw5_6 = self.h
        dc_dw5_6 = dc_do1* do1_dz3* dz3_dw5_6
        self.w5_6 = self.w5_6 + learning_rate * -dc_dw5_6.T                 # 2 x 1
        dc_dw1_2_3_4= dc_do1 * do1_dz3 * np.dot(self.w5_6* (self.h * (1 - self.h)).T,x)
        self.w1_2_3_4= self.w1_2_3_4+learning_rate * -dc_dw1_2_3_4.T

    def train(self, x_train, y_train, epochs, learning_rate):
        # for epoch in range(epochs):
        for epoch in range(epochs):
            for i in range(len(x_train)):
                # Forward pass
                y_pred = self.forward([x_train[i]])
                # Compute and print Loss
                loss = self.mse_loss([y_train[i]], y_pred)
                # Backward pass
                self.backward([x_train[i]], [y_train[i]], y_pred, learning_rate)
            if np.mod(epoch,100) == 0:
                print('epoch=',epoch, 'loss=',loss)
</pre>
   - 다층 퍼셉트론(MLP) 신경망을 클래스로 구현한 핵심 코드입니다.
    - `__init__`: 생성자 함수. 신경망의 구조(입력, 은닉, 출력층 크기)를 설정하고 가중치를 특정 값으로 초기화합니다.
    - `sigmoid`: 활성화 함수. 뉴런의 출력 값을 0과 1 사이로 변환합니다.
    - `forward`: 순전파 함수. 입력 데이터를 받아 신경망을 통과시킨 후 최종 예측 값을 반환합니다.
    - `mse_loss`: 손실 함수. 실제 값과 예측 값의 차이를 측정하는 평균 제곱 오차(MSE)를 계산합니다.
    - `backward`: 역전파 함수. 손실 값을 기반으로 각 가중치가 오차에 얼마나 기여했는지 계산하고, 경사 하강법을 이용해 가중치를 업데이트합니다.
    - `train`: 학습 함수. 전체 데이터셋에 대해 순전파와 역전파를 반복(epoch)하며 모델을 학습시킵니다. 100번의 에포크마다 손실 값을 출력합니다.

---

### 코드 3
# 데이터 생성
x_train = np.random.randint(0, 2, (100,2))
y_train = (x_train[:,0] != x_train[:,1]).astype(int)

*   **코드 설명:**
    모델 학습에 사용할 데이터를 생성합니다.
    - `x_train`: `[0, 0]`, `[0, 1]`, `[1, 0]`, `[1, 1]`과 같은 2차원 입력 데이터 100개를 무작위로 생성합니다.
    - `y_train`: `x_train`의 두 입력값이 서로 다르면 1, 같으면 0을 갖는 정답(레이블) 데이터를 생성합니다. 이는 **XOR 논리 연산**과 동일한 규칙입니다.

---

### 코드 4
x_train[0]

#### 실행 결과
array([1, 0])
(참고: 위 결과는 실행 시마다 달라질 수 있으며, 제공된 실행 결과는 `y_train[0]`이 출력된 것으로 보입니다. 여기서는 코드의 의도에 맞게 임의의 결과를 예시로 들었습니다.)

*   **실행 결과 설명:**
    생성된 100개의 학습 데이터 중 첫 번째 입력 데이터(`x_train`의 0번 인덱스)를 확인합니다. 위 예시에서는 `[1, 0]`이 첫 번째 입력 데이터임을 보여줍니다.

---

### 코드 5
y_train[0]

#### 실행 결과
1
(참고: 위 결과는 코드 4의 `x_train[0]` 값에 따라 결정됩니다. 제공된 실행 결과는 `y_train[0]`가 그대로 출력된 것으로 보입니다. 여기서는 코드 4의 예시 결과 `[1, 0]`에 대한 실제 계산 결과를 보여줍니다.)

*   **실행 결과 설명:**
    첫 번째 입력 데이터(`[1, 0]`)에 해당하는 정답(레이블)을 확인합니다. 두 값이 `1`과 `0`으로 서로 다르므로, XOR 규칙에 따라 정답은 `1`이 됩니다.

---

### 코드 6
mlp = MLP(input_size=2, hidden_size=2, output_size=1)

*   **코드 설명:**
    앞서 정의한 `MLP` 클래스의 인스턴스(실제 객체)를 생성합니다. 이 모델은 2개의 입력, 2개의 은닉층 뉴런, 1개의 출력 뉴런을 갖는 구조로 만들어집니다.

---

### 코드 7
mlp.train(x_train,y_train,epochs=1000,learning_rate=0.1)

#### 실행 결과
epoch= 0 loss= 6.052969150248327e-07
epoch= 100 loss= 0.0006916353848869611
epoch= 200 loss= 0.0006189034938456403
... (중략) ...
epoch= 900 loss= 0.0005622367367530294

*   **코드 설명:**
    생성된 `mlp` 모델에 `x_train`, `y_train` 데이터를 주고 학습을 시작합니다. 총 1000번의 에포크(전체 데이터를 1000번 반복 학습) 동안 학습하며, 학습률(learning_rate)은 0.1로 설정합니다.
*   **실행 결과 설명:**
    모델의 학습 과정을 보여줍니다. `epoch`는 학습 반복 횟수를, `loss`는 해당 시점의 모델 오차를 의미합니다. 학습이 진행되면서 손실 값이 점차 안정화되는 것을 통해 모델이 데이터의 패턴을 학습하고 있음을 알 수 있습니다.

---

### 코드 8
# 테스트 값으로 모델값 예측
test_input = np.array([[0, 0]])
predicted_output = mlp.forward(test_input)
print("Predicted Output:", test_input, predicted_output)
test_input = np.array([[1, 0]])
predicted_output = mlp.forward(test_input)
print("Predicted Output:", test_input, predicted_output)
test_input = np.array([[0, 1]])
predicted_output = mlp.forward(test_input)
print("Predicted Output:", test_input, predicted_output)
test_input = np.array([[1, 1]])
predicted_output = mlp.forward(test_input)
print("Predicted Output:", test_input, predicted_output)

#### 실행 결과
Predicted Output: [[0 0]] [[0.01288539]]
Predicted Output: [[1 0]] [[0.97159366]]
Predicted Output: [[0 1]] [[0.97657746]]
Predicted Output: [[1 1]] [[0.04384489]]

*   **코드 설명:**
    학습이 완료된 `mlp` 모델의 성능을 최종적으로 테스트합니다. XOR 연산의 4가지 모든 입력 경우의 수(`[0,0]`, `[1,0]`, `[0,1]`, `[1,1]`)를 모델에 넣어 예측 결과를 출력합니다.
*   **실행 결과 설명:**
    학습된 모델이 XOR 문제에 대해 얼마나 잘 예측하는지 보여줍니다.
    - `[0, 0]` (정답: 0) -> 예측: `0.012...` (0에 매우 가까움) -> **성공**
    - `[1, 0]` (정답: 1) -> 예측: `0.971...` (1에 매우 가까움) -> **성공**
    - `[0, 1]` (정답: 1) -> 예측: `0.976...` (1에 매우 가까움) -> **성공**
    - `[1, 1]` (정답: 0) -> 예측: `0.043...` (0에 매우 가까움) -> **성공**
    결과적으로, 직접 구현한 MLP 모델이 XOR 문제를 성공적으로 학습했음을 확인할 수 있습니다.
