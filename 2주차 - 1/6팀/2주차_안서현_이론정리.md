
1. 시그모이드 활성화 함수
  - 다층 신경망의 활성화 함수
    
    : 시그모이드 활성화 함수는 딥러닝처럼 복잡하고 다양한 기능을 갖는 신경망으로 발전할 수 있게 만든 초석이 되었다.
    
  - 시그모이드 함수의 장점
    
    : 퍼셉트론의 활성화 함수인 계단함수는 비선형 함수이다. 계단함수는 역치 이하의 값은 모두 0, 역치 이상의 값은 모두 1로 만들어 내부값의 차이를 비교할 수 없다.
    
    : 하지만 시그모이드 함수는 S자 형태의 함수로, 계단함수로는 표현할 수 없었던 내부값의 차이를 표현할 수 있다.
    
    : 시그모이드 함수는 신경망의 내부값에 따라 0과 1사이의 값을 출력한다. 이는 내부값을 확률로 변환해줌을 의미한다.
    
    : 시그모이드 함수가 미분가능한 함수라는 점도 있다. 다층 신경망은 오류 역전파 알고리즘을 사용한다. 이 알고리즘은 경사하강법을 사용하는데, 이는 활성화 함수가 미분가능해야한다.
    
  - 시그모이드 함수의 한계
    
    : 시그모이드 함수의 양 끝쪽의 기울기는 0에 가깝다. 이는 입력 값의 차이가 커도 출력값의 차이가 작음을 의미한다.
    
    : 그렇기에 내부값의 차이를 구분하지 못하는 계단함수의 한계를 시그모이드 함수도 가지고 있다. 입력값이 커지면서 기울기도 작아지고, 기울기가 0이 되면 학습효과가 떨어진다.
    
    : 시그모이드 함수는 항상 양의 값을 가지는 문제점도 있다. 가중치 업데이트 식을 통해 두 연결강도의 부호가 항상 동일하게 변한다. 이는 최적의 연결 가중치 값을 찾는데 지그재그로 움직임을 의미한다. 결국 학습과정의 능률이 떨어진다.
    
2. 시그모이드 함수의 한계 극복
   
  - tanh 함수
    
    : tanh 함수는 -1부터 1사이의 값을 가진다. -> 항상 양의 값을 가지는 문제 해결
    
    : 하지만 기울기 문제는 그대로이다.
    
  - ReLU 함수
    
    : 양수에서 함수의 기울기가 늘 1로 고정된다. -> 기울기 소실 문제 해결 ->오류역전파를 통한 학습이 가능하다.
    
    : 하지만 입력값이 음수인 경우 기울기가 0이 된다. -> Dying ReLU 문제 발생
    
  - ReLU 변형 함수
    
    : Dying ReLU 문제 해결을 위해 음수 입력값에 작은 기울기를 준다. -> Leaky ReLU
    
    : 음수의 기울기를 a라는 변수로 정해서 모델에 따라 변수값을 자유롭게 설정할 수 있게 했다. -> PreLU
    
    : 음수 값 부분을 곡선이 들어간 지수함수로 사용한다. -> ELU
    
3. 경사하강법
   
  - 편향이란?
    
    : 활성화 함수를 한쪽으로 옮겨 학습 속도를 높이는 것이다.
    
  - 손실함수란?
    
    : 신경망 모델의 예측 값과 실제 값 간의 차이를 측정하는 함수이다. (오차 측정)

    : 대표적인 손실함수로는 MSE 손실함수가 있다. 각 데이터들의 오차들의 제곱합의 평균이다.

    : MAE 손실함수도 있다. MSE에서 제곱 대신 절대값을 사용한 것이다.

    : 교차 엔트로피 함수도 있다. MSE보다 학습속도가 빠르다.
  
  - 경사하강법이란?
    
    : 주어진 손실함수에서 모델의 파라미터의 최적의 값을 찾는 머신러닝과 딥러닝의 최적화 알고리즘중 하나이다.

    : 오차가 가장 낮은 곳의 접선의 기울기가 0이다. 그래서 접선의 기울기를 0까지 점진적으로 하강시키는 방법이다.

    : 접선의 기울기가 음수이면 x를 증가, 접선의 기울기가 양수이면 x를 감소시킨다. 기울기가 클수록 그 변화량도 커진다.


4. 역전파 알고리즘
   
  - 역전파 알고리즘이란?
    
    : 신경망에서 오차를 출력층에서부터 입력층으로 거꾸로 전달하면서 각 가중치의 기울기를 계산하는 알고리즘이다. 이걸 통해 각 가중치를 얼마나, 어떤 방향으로 업데이트할지 결정할 수 있다.
  
  - 알고리즘 흐름
    
  - 순전파 (Forward Propagation)
    
    : 입력 → 은닉층 → 출력까지 계산해서 예측값을 구한다.
  
  - 오차 계산
    
    : 손실함수로 예측값과 실제값 사이의 오차를 계산한다.
  
  - 역전파
    
    : 오차를 출력층 → 은닉층 → 입력층 방향으로 전달하면서, 각 가중치에 대한 오차의 기울기(미분값)를 계산한다.
  
  - 가중치 업데이트
    
    : 계산된 기울기를 사용해 경사 하강법으로 가중치를 조정
  
  - 체인 룰
    
    : 원하는 편미분 값을 구하기 위해 알고있는 편미분 값들을 연속적으로 곱하는 것이다.


5. 파이토치(PyTorch)의 아키텍처
   
  - 전반적인 구조
    
    : PyTorch는 크게 세 개의 계층으로 구성됩니다:

    : PyTorch API 계층: 사용자 친화적인 인터페이스 제공 (ex. torch, torch.nn 등)

    : PyTorch 엔진: 다차원 텐서 처리, 자동 미분, 연산 최적화

    : 연산 처리 계층: C/CUDA 기반으로 실제 CPU/GPU 연산 수행
  
  - PyTorch API 계층
    
    : 사용자가 모델을 설계하고 학습할 수 있도록 다양한 패키지 제공

    : 실제 연산은 하지 않고 PyTorch 엔진에 연산 요청을 전달
  
  - 주요 패키지:
    
    : torch: 다차원 텐서 및 연산, GPU 지원

    : torch.autograd: 자동 미분을 통한 연산 그래프 구축

    : torch.nn: 신경망 구성 (CNN, RNN, 정규화 등)

    : torch.multiprocessing: 텐서 메모리 공유 지원

    : torch.utils: DataLoader, bottleneck, checkpoint 등 유틸리티 제공
  

  - PyTorch 엔진
    
    : C++ 기반으로 성능 최적화
  
  - 구성 요소
    
    : Autograd C++: 자동 미분 처리

    : Aten C++: 텐서 연산 라이브러리

    : JIT C++: Just-In-Time 컴파일을 통한 연산 최적화

    : Python API로 wrapping 되어 사용자 접근 가능
  

  - 연산 처리 계층
    
    : 실제 연산을 담당하는 C/CUDA 라이브러리 (CPU: TH, GPU: THC 등)

    : 다차원 텐서의 수학적 연산을 효율적으로 처리


6. 텐서 메모리 구조와 저장 방식
   
  - 텐서의 저장 방식
    
    : 텐서는 어떤 차원이든 메모리에는 1차원 배열로 저장됨

    : 이 1차원 배열을 **스토리지(Storage)**라고 부름
  
  - 오프셋(Offset)과 스트라이드(Stride)
    
    : Offset: 텐서의 첫 번째 요소가 스토리지에서 시작하는 위치

    : Stride: 다음 요소로 이동하기 위해 건너뛰는 메모리 공간 크기

  - 오프셋/스트라이드의 중요성
    
    : 텐서가 같은 값을 가져도 메모리 배치(형태)는 다를 수 있음

    : 전치(transpose) 연산도 실제 값은 변하지 않지만 스트라이드와 형태가 바뀜 -> 이를 통해 텐서의 뷰(view)를 효율적으로 관리할 수 있음



