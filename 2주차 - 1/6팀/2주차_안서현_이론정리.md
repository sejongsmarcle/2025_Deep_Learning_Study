1. 시그모이드 활성화 함수
  - 다층 신경망의 활성화 함수
    : 시그모이드 활성화 함수는 딥러닝처럼 복잡하고 다양한 기능을 갖는 신경망으로 발전할 수 있게 만든 초석이 되었다.
  - 시그모이드 함수의 장점
    : 퍼셉트론의 활성화 함수인 계단함수는 비선형 함수이다. 계단함수는 역치 이하의 값은 모두 0, 역치 이상의 값은 모두 1로 만들어 내부값의 차이를 비교할 수 없다.
    : 하지만 시그모이드 함수는 S자 형태의 함수로, 계단함수로는 표현할 수 없었던 내부값의 차이를 표현할 수 있다.
    : 시그모이드 함수는 신경망의 내부값에 따라 0과 1사이의 값을 출력한다. 이는 내부값을 확률로 변환해줌을 의미한다.       : 시그모이드 함수가 미분가능한 함수라는 점도 있다. 다층 신경망은 오류 역전파 알고리즘을 사용한다. 이 알고리즘은 경사하강법을 사용하는데, 이는 활성화 함수가 미분가능해야한다.
  - 시그모이드 함수의 한계
    : 시그모이드 함수의 양 끝쪽의 기울기는 0에 가깝다. 이는 입력 값의 차이가 커도 출력값의 차이가 작음을 의미한다.
    : 그렇기에 내부값의 차이를 구분하지 못하는 계단함수의 한계를 시그모이드 함수도 가지고 있다. 입력값이 커지면서 기울기도 작아지고, 기울기가 0이 되면 학습효과가 떨어진다.
    : 시그모이드 함수는 항상 양의 값을 가지는 문제점도 있다. 가중치 업데이트 식을 통해 두 연결강도의 부호가 항상 동일하게 변한다. 이는 최적의 연결 가중치 값을 찾는데 지그재그로 움직임을 의미한다. 결국 학습과정의 능률이 떨어진다.
    
2. 시그모이드 함수의 한계 극복
  - tanh 함수
    : tanh 함수는 -1부터 1사이의 값을 가진다. -> 항상 양의 값을 가지는 문제 해결
    : 하지만 기울기 문제는 그대로이다.
  - ReLU 함수
    : 양수에서 함수의 기울기가 늘 1로 고정된다. -> 기울기 소실 문제 해결 ->오류역전파를 통한 학습이 가능하다.
    : 하지만 입력값이 음수인 경우 기울기가 0이 된다. -> Dying ReLU 문제 발생
  - ReLU 변형 함수
    : Dying ReLU 문제 해결을 위해 음수 입력값에 작은 기울기를 준다. -> Leaky ReLU
    : 음수의 기울기를 a라는 변수로 정해서 모델에 따라 변수값을 자유롭게 설정할 수 있게 했다. -> PreLU
    : 음수 값 부분을 곡선이 들어간 지수함수로 사용한다. -> ELU
    
3. 경사하강법
  - 편향이란?
    : 활성화 함수를 한쪽으로 옮겨 학습 속도를 높이는 것이다.
  - 손실함수란?
    : 신경망 모델의 예측 값과 실제 값 간의 차이를 측정하는 함수이다. (오차 측정)
    : 대표적인 손실함수로는 MSE 손실함수가 있다. 각 데이터들의 오차들의 제곱합의 평균이다.
    : MAE 손실함수도 있다. MSE에서 제곱 대신 절대값을 사용한 것이다.
    : 교차 엔트로피 함수도 있다. MSE보다 학습속도가 빠르다.
  - 경사하강법이란?
    : 주어진 손실함수에서 모델의 파라미터의 최적의 값을 찾는 머신러닝과 딥러닝의 최적화 알고리즘중 하나이다.
    : 오차가 가장 낮은 곳의 접선의 기울기가 0이다. 그래서 접선의 기울기를 0까지 점진적으로 하강시키는 방법이다.
    : 접선의 기울기가 음수이면 x를 증가, 접선의 기울기가 양수이면 x를 감소시킨다. 기울기가 클수록 그 변화량도 커진다.

4. 역전파 알고리즘
  - 역전파 알고리즘이란?
    : 신경망에서 오차를 출력층에서부터 입력층으로 거꾸로 전달하면서 각 가중치의 기울기를 계산하는 알고리즘이다. 이걸 통해 각 가중치를 얼마나, 어떤 방향으로 업데이트할지 결정할 수 있다.
  - 알고리즘 흐름
  - 순전파 (Forward Propagation)
    : 입력 → 은닉층 → 출력까지 계산해서 예측값을 구한다.
  - 오차 계산
    : 손실함수로 예측값과 실제값 사이의 오차를 계산한다.
  - 역전파
    : 오차를 출력층 → 은닉층 → 입력층 방향으로 전달하면서, 각 가중치에 대한 오차의 기울기(미분값)를 계산한다.
  - 가중치 업데이트
    : 계산된 기울기를 사용해 경사 하강법으로 가중치를 조정
  - 체인 룰
    : 원하는 편미분 값을 구하기 위해 알고있는 편미분 값들을 연속적으로 곱하는 것이다.

