1. 인공신경망
  - 인경신공망이란?
    : 인공지능의 핵심인 딥러닝 기술의 기초로, 인공지능을 사람의 뇌처럼 작동시키려는 프로그램이다.
    : 인간의 뇌는 수많은 뉴런의 연결로 구성되어있다. 이 뉴런은 입력, 계산, 출력 기능을 가진 정보처리 기간이다.
    : 인공신경세포는 위 뉴련의 정보처리 방식을 모방하는 방식으로 구현한다. 인공신경세포들을 모아 인공신경망을 구현한다.
    : 인공신경망은 원들과 원을 연결하는 선들로 구성된다.
    : 원은 인공 뉴련이나 노드라고 불리며, 선은 연결가중치나 연결강도 혹은 웨이트라 불린다.
    : 원하는 결과를 얻기위해 연결가중치의 값을 조절하는데, 이것을 신경망의 학습이라 한다.
  - 인공신경망의 작동원리
    : 노드는 고유한 연결강도를 가진 여러 입력를 합해 노드값을 계산한다. 그 후 다음 뉴런에게 계산값을 전달한다.
    : 뉴런에서 역치 이상의 값이 올때 발화하듯이, 노드에 활성화 함수를 붙여 출력값을 계산한다.
    : 활성화 함수는 매우 다양하게 설정될 수 있다. 간단한 예시로는 역치값 이상일때 1, 이하일때 0일 출력하는 활성화 함수가 있다.
    : 이것이 딥러닝 모델의 학습 알고리즘의 기초인 퍼셉트론의 기본 작동 방식이다.
  - 인공신경망의 학습
    : 원하는 결과를 얻기 위해 연결강도의 값을 조절하는데, 이것을 신경망의 학습이라 한다.
    : 여러 값들을 정리한 것을 데이터 셋이라 한다.
    : 데이터 셋을 인공신경망에 넣어 나온 값과, 실제 값을 비교하여 현재 인공신경망의 오차를 계산한다.
    : 오차를 줄이기 위해 연결강도를 새로 설정해야 한다.
    : 퍼셉트론의 학습 알고리즘 : 새 연결강도 = 현 연결강도 + 현 입력값 X 오차 X 학습률
    : 위 식을 통해, 오차가 클수록 새 연결강도는 더 크게 변한다.
    : 학습률은 작은 값으로 설정해서 연결강도를 천천히 변화시킨다.
    : 위 과정을 거친 후, 해당 인공신경망은 더 높은 적중률을 가질 것이다.

2. 퍼셉트론의 한계
  - 퍼셉트론의 한계
    : 퍼셉트론은 선형분류기로, 선형분리가 가능한 데이터들을 분류할 수 있다.
    : n개의 입력이 있을때, n - 1개 차원의 초 평면으로 분리한다.
    : 하지만 데이터 셋에 대한 선형분리가 불가능하다면, 퍼셉트론의 학습은 불가능하다. (예시: XOR 게이트 문제)
    : 이런 간단한 문제도 해결할 수 없다는 문제점은 인공신경망의 첫번째 빙하기를 시작했다.

3. 다층 신경망
  - 다층 신경망의 등장
    : 기존 퍼셉트론보다 강력하며, 인공신경망의 부흥을 이끌었다.
    : 여러 퍼셉트론을 연결하여 데이터 셋을 비선형으로 분리할 수 있는 다층 신경망을 구현한다.
    : 다층 신경망은 하나의 입력층과 한 개 이상의 은닉층과 출력층으로 이루어진다.
    : 층이 많을 수 록 더 복잡한 데이터 셋을 다룰 수 있으며, 층이 많다는 것을 deep하다고 표현하여 현재의 딥러닝이라는 명칭이 탄생했다.
    : 다층 신경망의 활성화 함수는 더욱 복잡해졌으며, 경사하강법이나 역전파 알고리즘등 다층 신경망의 오차를 줄이기 위한 여러 방법도 등장한다.
  - 다층 신경망의 구체적인 학습 과정
    : 순전파 (Feed Forward):
      처음에는 각 연결의 가중치를 임의의 값으로 설정합니다.
      입력층에서부터 은닉층을 거쳐 출력층 방향으로 계산을 수행하여 예측값을 출력합니다.
      이 예측값과 실제 정답 값을 비교하여 오차를 계산합니다.
    : 역전파 (Back Propagation):
      계산된 오차를 출력층에서부터 입력층 방향으로 거꾸로 전파시킵니다.
      오차를 줄이는 방향으로 각 연결의 가중치를 조금씩 업데이트합니다.
    : 반복 학습과 세대 (Epoch):
      신경망은 '순전파'와 '역전파' 과정을 계속 반복하면서 최적의 가중치를 찾아갑니다.
      가지고 있는 모든 데이터 셋에 대해 순전파와 역전파를 한 번씩 완료한 것을 1 세대(Epoch)라고 부릅니다.
      수많은 Epoch을 반복하여 학습을 진행하면, 오차가 최소화된 가중치를 찾을 수 있습니다.
